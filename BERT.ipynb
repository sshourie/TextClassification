{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6s-Tz5j3Esuo"},"outputs":[],"source":["# !pip install transformers numpy pandas datasets scipy evaluate sklearn\n","# !pip install scikit-learn\n","# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","# !pip install accelerate -U"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":80,"status":"ok","timestamp":1700071248788,"user":{"displayName":"Suraj Shourie","userId":"09823311322610219849"},"user_tz":300},"id":"6mCQ9jVjEyQ7"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import transformers\n","from datasets import load_dataset\n","from transformers import AutoModelForSequenceClassification\n","from transformers import TFAutoModelForSequenceClassification\n","from transformers import AutoTokenizer, AutoConfig, TrainingArguments\n","from scipy.special import softmax\n","import torch\n","from transformers import DataCollatorWithPadding\n","import evaluate\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","1\n","cuda\n"]}],"source":["print(torch.cuda.is_available())\n","print(torch.cuda.device_count())\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2455,"status":"ok","timestamp":1700071253549,"user":{"displayName":"Suraj Shourie","userId":"09823311322610219849"},"user_tz":300},"id":"gMPcw6WfE3D0"},"outputs":[],"source":["imdb = load_dataset('imdb')\n","train,test = pd.DataFrame(imdb['train']), pd.DataFrame(imdb['test']) # not needed"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1470,"status":"ok","timestamp":1700071257625,"user":{"displayName":"Suraj Shourie","userId":"09823311322610219849"},"user_tz":300},"id":"-uRlkaffHC40","outputId":"148f6cdc-032a-4d86-a535-1881bfefddb3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\" # can choose some other package from Huggingface instead\n","MODEL = f\"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","# config = AutoConfig.from_pretrained(MODEL)\n","# load pre-trained model\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"kZlW1h0sMNh1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79c53de43e904af993749e04717a5864","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\github\\TextClassification\\TextClassification1.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m  truncate sequences to match the maximum input length\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tokenized_imdb \u001b[39m=\u001b[39m imdb\u001b[39m.\u001b[39;49mmap(preprocess_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\datasets\\dataset_dict.py:855\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    853\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    854\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 855\u001b[0m     {\n\u001b[0;32m    856\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    857\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    858\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    859\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    860\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    861\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    862\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    863\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    864\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    865\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    866\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    867\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    868\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    869\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    870\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    871\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    872\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    873\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    874\u001b[0m         )\n\u001b[0;32m    875\u001b[0m         \u001b[39mfor\u001b[39;49;00m k, dataset \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems()\n\u001b[0;32m    876\u001b[0m     }\n\u001b[0;32m    877\u001b[0m )\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\datasets\\dataset_dict.py:856\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    853\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    854\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    855\u001b[0m     {\n\u001b[1;32m--> 856\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    857\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    858\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    859\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    860\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    861\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    862\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    863\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    864\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    865\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    866\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    867\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    868\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    869\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    870\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    871\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    872\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    873\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    874\u001b[0m         )\n\u001b[0;32m    875\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    876\u001b[0m     }\n\u001b[0;32m    877\u001b[0m )\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:591\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    590\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    592\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    593\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    594\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:556\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    550\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    554\u001b[0m }\n\u001b[0;32m    555\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    557\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    558\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3089\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3082\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3083\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3084\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3085\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3086\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[0;32m   3087\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3088\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3089\u001b[0m         \u001b[39mfor\u001b[39;49;00m rank, done, content \u001b[39min\u001b[39;49;00m Dataset\u001b[39m.\u001b[39;49m_map_single(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdataset_kwargs):\n\u001b[0;32m   3090\u001b[0m             \u001b[39mif\u001b[39;49;00m done:\n\u001b[0;32m   3091\u001b[0m                 shards_done \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3466\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3462\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   3463\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   3464\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3465\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3466\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3467\u001b[0m         batch,\n\u001b[0;32m   3468\u001b[0m         indices,\n\u001b[0;32m   3469\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   3470\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m   3471\u001b[0m     )\n\u001b[0;32m   3472\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3473\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3474\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3475\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3345\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3343\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   3344\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 3345\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[0;32m   3346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3347\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m   3348\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[0;32m   3349\u001b[0m     }\n","\u001b[1;32md:\\github\\TextClassification\\TextClassification1.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_function\u001b[39m(examples):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m  truncate sequences to match the maximum input length\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}],"source":["def preprocess_function(examples):\n","  '''\n","  truncate sequences to match the maximum input length\n","  '''\n","  return tokenizer(examples[\"text\"], truncation=True)\n","tokenized_imdb = imdb.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\github\\TextClassification\\TextClassification1.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m accuracy \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}],"source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","accuracy = evaluate.load(\"accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"d87wzNG5JMFK"},"source":["### Untrained Model\n","Accuracy\n","https://huggingface.co/docs/transformers/tasks/sequence_classification"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# maybe need a GPU running for all test failes, or batch it properly\n","\n","# preds = []\n","# batch_size = 100\n","# n = len(test)//batch_size\n","# n = 2\n","# for i in range(n+1):\n","#   # if (i+1) % (n // 10) == 0:\n","#   #   print(f'{round((i+1)*100/n,0)}% complete...')\n","#   text = list(test.iloc[i*batch_size:(i+1)*batch_size,0])\n","#   encoded_input = tokenizer(list(text), return_tensors='pt', padding=True, truncation=True) # need to give padding and truncation as true\n","#   with torch.no_grad():\n","#     output = model(**encoded_input)\n","#   preds.append(output.logits.detach().argmax(axis=1))"]},{"cell_type":"markdown","metadata":{},"source":["Looping through each data-point not the best solution// batch it and use GPU instead"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3197,"status":"ok","timestamp":1700071552427,"user":{"displayName":"Suraj Shourie","userId":"09823311322610219849"},"user_tz":300},"id":"QVS86CiWHRMZ","outputId":"fa9d1947-2fba-44db-950c-b78808c5300e"},"outputs":[],"source":["# preds = []\n","# n = len(test)\n","# for i in range(n):\n","#   if (i+1) % (n // 10) == 0:\n","#     print(f'{round((i+1)*100/n,0)}% complete...')\n","#   text = test.iloc[i,0]\n","#   encoded_input = tokenizer(str(text), return_tensors='pt', padding=True, truncation=True) # need to give padding and truncation as true\n","#   with torch.no_grad():\n","#     output = model(**encoded_input)\n","#   preds.append(output[0][0].detach().numpy().argmax())"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2103,"status":"ok","timestamp":1700071561729,"user":{"displayName":"Suraj Shourie","userId":"09823311322610219849"},"user_tz":300},"id":"gCG2Dw7uKH6O","outputId":"8ea3a826-a291-42e2-f871-8adb067135c8"},"outputs":[],"source":["# accuracy_score(test.label, preds)"]},{"cell_type":"markdown","metadata":{},"source":["Finetuning model"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3abb30f2babf42b8baa5249a3775ad20","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","# !pip install ipywidgets\n","notebook_login()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70af606ad8e54ded881161697f1e1caa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/9375 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/plain":["TrainOutput(global_step=9375, training_loss=0.19898288045247395, metrics={'train_runtime': 25886.29, 'train_samples_per_second': 2.897, 'train_steps_per_second': 0.362, 'train_loss': 0.19898288045247395, 'epoch': 3.0})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import TrainingArguments, Trainer\n","training_args = TrainingArguments(output_dir=\"test_trainer\", push_to_hub=True) # default params \n","# possible args\n","# training_args = TrainingArguments(\n","#     output_dir=output_dir,\n","#     learning_rate=2e-5,\n","#     per_device_train_batch_size=16,\n","#     per_device_eval_batch_size=16,\n","#     num_train_epochs=2,\n","#     weight_decay=0.01,\n","#     evaluation_strategy=\"epoch\",\n","#     save_strategy=\"epoch\",\n","#     load_best_model_at_end=True,\n","#     push_to_hub=True,\n","# )\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_imdb[\"train\"],\n","    eval_dataset=tokenized_imdb[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":[" inference on fine-tuned model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["MODEL = f\"sshourie/test_trainer\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","# config = AutoConfig.from_pretrained(MODEL)\n","# load pre-trained model\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10.0% complete...\n","20.0% complete...\n","30.0% complete...\n","40.0% complete...\n","50.0% complete...\n","60.0% complete...\n","70.0% complete...\n","80.0% complete...\n","90.0% complete...\n","100.0% complete...\n"]},{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32md:\\github\\TextClassification\\TextClassification1.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m((i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m/\u001b[39mn,\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m% complete...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(test\u001b[39m.\u001b[39miloc[i\u001b[39m*\u001b[39mbatch_size:(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mbatch_size,\u001b[39m0\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(\u001b[39mlist\u001b[39;49m(text), return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m# need to give padding and truncation as true\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/github/TextClassification/TextClassification1.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_input)\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2798\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[0;32m   2799\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2880\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2881\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2882\u001b[0m         )\n\u001b[0;32m   2883\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[0;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[0;32m   2886\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2887\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2888\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   2889\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2890\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2891\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   2892\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2893\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2894\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   2895\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   2896\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   2897\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   2898\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   2899\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   2900\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2901\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2902\u001b[0m     )\n\u001b[0;32m   2903\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2904\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2905\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2906\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2922\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2923\u001b[0m     )\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3065\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3067\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   3068\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3072\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3073\u001b[0m )\n\u001b[1;32m-> 3075\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[0;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[0;32m   3077\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   3078\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m   3079\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m   3080\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   3081\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   3082\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   3083\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   3084\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   3085\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   3086\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   3087\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   3088\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   3089\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   3090\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   3091\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   3092\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   3093\u001b[0m )\n","File \u001b[1;32md:\\github\\TextClassification\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:537\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[39m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[39m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[39m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[0;32m    536\u001b[0m sanitized_tokens \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 537\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m tokens_and_encodings[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    538\u001b[0m     stack \u001b[39m=\u001b[39m [e \u001b[39mfor\u001b[39;00m item, _ \u001b[39min\u001b[39;00m tokens_and_encodings \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m item[key]]\n\u001b[0;32m    539\u001b[0m     sanitized_tokens[key] \u001b[39m=\u001b[39m stack\n","\u001b[1;31mIndexError\u001b[0m: list index out of range"]}],"source":["preds = []\n","batch_size = 100\n","n = len(test)//batch_size\n","# n = 2\n","for i in range(n+1):\n","  if (i+1) % (n // 10) == 0:\n","    print(f'{round((i+1)*100/n,0)}% complete...')\n","  text = list(test.iloc[i*batch_size:(i+1)*batch_size,0])\n","  if len(text) == 0:\n","    break\n","  encoded_input = tokenizer(list(text), return_tensors='pt', padding=True, truncation=True) # need to give padding and truncation as true\n","  with torch.no_grad():\n","    output = model(**encoded_input)\n","  preds.append(output.logits.detach().argmax(axis=1))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.93648\n"]}],"source":["print(accuracy_score(test.label, torch.concat(preds)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(accuracy_score(test.label, preds))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOt+2L7LFN8c852HG4jn0QE","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
